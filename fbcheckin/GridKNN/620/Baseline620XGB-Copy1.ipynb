{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "mingw_path = 'C:\\\\mingw-w64\\\\mingw64\\\\bin'\n",
    "os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_list = ['x','y','hour','weekday', 'day','month','year', 'accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(df):    \n",
    "    #Feature engineering\n",
    "    df.x = df.x.values\n",
    "    df.y = df.y.values\n",
    "    initial_date = np.datetime64('2014-01-02T01:01', dtype='datetime64[m]') \n",
    "    d_times = pd.DatetimeIndex(initial_date + np.timedelta64(int(mn), 'm') \n",
    "                               for mn in df.time.values)    \n",
    "    df['hour'] = (d_times.hour+ d_times.minute/60)\n",
    "    #for i in range(0,24):\n",
    "    #    df['h' + str(i)] = (((d_times.hour+ d_times.minute/60) + i) % 24)\n",
    "    \n",
    "    #df['w0'] = ((d_times.weekday + 0) % 7)\n",
    "    #df['w1'] = ((d_times.weekday + 1) % 7)\n",
    "    #df['w2'] = ((d_times.weekday + 2) % 7)\n",
    "    #df['w3'] = ((d_times.weekday + 3) % 7)\n",
    "    #df['w4'] = ((d_times.weekday + 4) % 7)\n",
    "    #df['w5'] = ((d_times.weekday + 5) % 7)\n",
    "    #df['w6'] = ((d_times.weekday + 6) % 7)\n",
    "    df['weekday'] = d_times.weekday\n",
    "    \n",
    "    df['day'] = (d_times.dayofyear).astype(int)\n",
    "    df['month'] = d_times.month\n",
    "    df['year'] = (d_times.year - 2013)\n",
    "    #df.accuracy = df.accuracy.values * fw[7]\n",
    "    df['accuracy'] = np.log10(df.accuracy)\n",
    "    df['log_month'] = np.log10(3+df.time/(60 * 24 * 30))\n",
    "    #df = df.drop(['time'], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading csv files from disk.\n",
      "Calculating time features\n",
      "Binning data\n",
      "Starting CV\n",
      "Bin 38,14\n",
      "[0]\ttrain-merror:0.126267+0.00357539\ttest-merror:0.178857+0.00648689\n",
      "[10]\ttrain-merror:0.101079+0.00377701\ttest-merror:0.171737+0.00828306\n",
      "[20]\ttrain-merror:0.0892126+0.00178405\ttest-merror:0.169148+0.0105121\n",
      "[30]\ttrain-merror:0.0782092+0.00124187\ttest-merror:0.164617+0.0114693\n",
      "[40]\ttrain-merror:0.0676914+0.00146723\ttest-merror:0.164401+0.0105388\n",
      "[50]\ttrain-merror:0.0576592+0.000711339\ttest-merror:0.164186+0.00634188\n",
      "[60]\ttrain-merror:0.0497306+0.00203702\ttest-merror:0.16397+0.00491968\n",
      "[70]\ttrain-merror:0.0401832+0.0019147\ttest-merror:0.163538+0.00494822\n",
      "    test-merror-mean  test-merror-std  train-merror-mean  train-merror-std\n",
      "0           0.178857         0.006487           0.126267          0.003575\n",
      "1           0.177562         0.004948           0.119417          0.002846\n",
      "2           0.176268         0.004948           0.115318          0.001659\n",
      "3           0.175405         0.005311           0.113107          0.000652\n",
      "4           0.176915         0.005241           0.110410          0.000863\n",
      "5           0.173678         0.007220           0.108306          0.001641\n",
      "6           0.172168         0.007616           0.107012          0.002360\n",
      "7           0.171521         0.008522           0.105070          0.002837\n",
      "8           0.171737         0.008367           0.103560          0.002351\n",
      "9           0.173031         0.006877           0.102157          0.002390\n",
      "10          0.171737         0.008283           0.101079          0.003777\n",
      "11          0.171305         0.009682           0.099569          0.002891\n",
      "12          0.171737         0.008395           0.098706          0.002472\n",
      "13          0.171306         0.010925           0.097519          0.002590\n",
      "14          0.169795         0.009735           0.096170          0.001413\n",
      "15          0.170227         0.009826           0.095415          0.001701\n",
      "16          0.168932         0.008119           0.094229          0.001249\n",
      "17          0.169795         0.008289           0.092718          0.000789\n",
      "18          0.167422         0.009240           0.091424          0.001731\n",
      "19          0.168500         0.009802           0.090723          0.001392\n",
      "20          0.169148         0.010512           0.089213          0.001784\n",
      "21          0.167638         0.009067           0.088350          0.001453\n",
      "22          0.167206         0.010503           0.087055          0.001199\n",
      "23          0.166775         0.010133           0.086300          0.001876\n",
      "24          0.168069         0.010946           0.085113          0.001371\n",
      "25          0.167206         0.011417           0.084250          0.001087\n",
      "26          0.167206         0.012109           0.082848          0.000789\n",
      "27          0.165480         0.012159           0.081230          0.000860\n",
      "28          0.164833         0.012136           0.080798          0.000755\n",
      "29          0.165480         0.011651           0.079666          0.000735\n",
      "..               ...              ...                ...               ...\n",
      "38          0.164401         0.010670           0.069741          0.002100\n",
      "39          0.164401         0.010670           0.068716          0.001451\n",
      "40          0.164401         0.010539           0.067691          0.001467\n",
      "41          0.163970         0.010050           0.066667          0.001292\n",
      "42          0.164186         0.009389           0.065588          0.001539\n",
      "43          0.164185         0.009658           0.064185          0.001079\n",
      "44          0.163754         0.008614           0.063646          0.001065\n",
      "45          0.165049         0.008603           0.062513          0.000731\n",
      "46          0.164186         0.008255           0.061704          0.000523\n",
      "47          0.165049         0.007505           0.060949          0.000703\n",
      "48          0.165049         0.006253           0.059655          0.001223\n",
      "49          0.164833         0.005924           0.058306          0.000691\n",
      "50          0.164186         0.006342           0.057659          0.000711\n",
      "51          0.163538         0.006566           0.056688          0.000956\n",
      "52          0.164401         0.005893           0.056149          0.001174\n",
      "53          0.164186         0.005214           0.055448          0.000974\n",
      "54          0.163754         0.005258           0.054477          0.001671\n",
      "55          0.163538         0.006349           0.053506          0.001736\n",
      "56          0.163538         0.006775           0.052913          0.001879\n",
      "57          0.163538         0.005971           0.051996          0.001925\n",
      "58          0.163754         0.006268           0.051133          0.001550\n",
      "59          0.163754         0.005432           0.050108          0.001520\n",
      "60          0.163970         0.004920           0.049731          0.002037\n",
      "61          0.164401         0.004556           0.048706          0.002100\n",
      "62          0.164617         0.004657           0.047789          0.001985\n",
      "63          0.163754         0.005169           0.046872          0.001985\n",
      "64          0.163754         0.005214           0.045793          0.002272\n",
      "65          0.163538         0.005484           0.044660          0.001919\n",
      "66          0.163107         0.004747           0.043851          0.002044\n",
      "67          0.162675         0.005169           0.042880          0.002204\n",
      "\n",
      "[68 rows x 4 columns]\n",
      "Train Cross Validated MAP@3: 0.9723\n",
      "Bin 28,42\n",
      "[0]\ttrain-merror:0.155913+0.00569218\ttest-merror:0.217699+0.0115425\n",
      "[10]\ttrain-merror:0.122084+0.00259752\ttest-merror:0.197587+0.0132054\n",
      "[20]\ttrain-merror:0.106758+0.00471908\ttest-merror:0.193242+0.01212\n",
      "[30]\ttrain-merror:0.0930008+0.00357681\ttest-merror:0.191794+0.0108961\n",
      "[40]\ttrain-merror:0.0808126+0.00329352\ttest-merror:0.188415+0.00993661\n",
      "[50]\ttrain-merror:0.06963+0.00321158\ttest-merror:0.188254+0.0108293\n",
      "    test-merror-mean  test-merror-std  train-merror-mean  train-merror-std\n",
      "0           0.217699         0.011542           0.155913          0.005692\n",
      "1           0.209976         0.008813           0.144368          0.004608\n",
      "2           0.206275         0.008182           0.139582          0.004331\n",
      "3           0.201770         0.011562           0.134835          0.004685\n",
      "4           0.201609         0.012978           0.132140          0.004340\n",
      "5           0.200965         0.013437           0.130290          0.002995\n",
      "6           0.199839         0.012948           0.128842          0.003615\n",
      "7           0.199196         0.012334           0.126750          0.002574\n",
      "8           0.198069         0.012056           0.124980          0.002517\n",
      "9           0.199356         0.011024           0.123652          0.003298\n",
      "10          0.197587         0.013205           0.122084          0.002598\n",
      "11          0.198552         0.013359           0.120636          0.002910\n",
      "12          0.196621         0.013097           0.119067          0.003656\n",
      "13          0.197586         0.013715           0.117096          0.003533\n",
      "14          0.195173         0.012490           0.115929          0.003287\n",
      "15          0.194851         0.011937           0.114722          0.003716\n",
      "16          0.195173         0.010753           0.112671          0.004199\n",
      "17          0.194529         0.011416           0.111866          0.004721\n",
      "18          0.194208         0.012736           0.110137          0.003940\n",
      "19          0.194047         0.012226           0.108648          0.004863\n",
      "20          0.193242         0.012120           0.106758          0.004719\n",
      "21          0.192920         0.012332           0.105270          0.004389\n",
      "22          0.191794         0.011797           0.103942          0.004229\n",
      "23          0.190990         0.011270           0.102414          0.003637\n",
      "24          0.191311         0.011210           0.100443          0.003880\n",
      "25          0.191794         0.011224           0.099397          0.003551\n",
      "26          0.191794         0.010460           0.097546          0.003571\n",
      "27          0.192116         0.010894           0.097023          0.003792\n",
      "28          0.192277         0.011240           0.095374          0.003364\n",
      "29          0.191794         0.010753           0.093886          0.003542\n",
      "30          0.191794         0.010896           0.093001          0.003577\n",
      "31          0.191955         0.010509           0.091754          0.003498\n",
      "32          0.190990         0.010082           0.090346          0.003942\n",
      "33          0.191633         0.009976           0.088898          0.003827\n",
      "34          0.191311         0.010245           0.087611          0.003924\n",
      "35          0.190668         0.010227           0.086887          0.003843\n",
      "36          0.190185         0.009887           0.085640          0.003650\n",
      "37          0.189863         0.010829           0.084714          0.003588\n",
      "38          0.189380         0.010570           0.083146          0.003214\n",
      "39          0.188898         0.010370           0.081537          0.002960\n",
      "40          0.188415         0.009937           0.080813          0.003294\n",
      "41          0.189381         0.009568           0.079405          0.003138\n",
      "42          0.189059         0.010661           0.078439          0.003160\n",
      "43          0.188415         0.009249           0.077112          0.002908\n",
      "44          0.189059         0.010138           0.076066          0.002726\n",
      "45          0.187772         0.009608           0.075221          0.002430\n",
      "46          0.186967         0.010017           0.073492          0.003060\n",
      "Train Cross Validated MAP@3: 0.9575\n",
      "MAP@3 CV in bins [0.97229409, 0.95751578]\n",
      "Writing xgb_submission.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "from sklearn import preprocessing\n",
    "\n",
    "NBINS=2 # Randomly pick this number of bins and fit\n",
    "# See line 92 for iterating over the whole dataset\n",
    "\n",
    "def horizontally_bin_data(data, NX, NY):\n",
    "    \"\"\"Add columns to data indicating X and Y bins.\n",
    "\n",
    "    Divides the grid into `NX` bins in X and `NY` bins in Y, and adds columns \n",
    "    to `data` containing the bin number in X and Y. \n",
    "    \"\"\"\n",
    "\n",
    "    NX = int(NX)\n",
    "    NY = int(NY)\n",
    "\n",
    "    assert((NX >= 5) and (NX <= 1000))\n",
    "    assert((NY >= 5) and (NY <= 1000))\n",
    "\n",
    "    x_bounds = (0., 10.)\n",
    "    y_bounds = (0., 10.)\n",
    "\n",
    "    delta_X = (x_bounds[1] - x_bounds[0]) / float(NX)\n",
    "    delta_Y = (y_bounds[1] - y_bounds[0]) / float(NY)\n",
    "\n",
    "    # very fast binning algorithm, just divide by delta and round down\n",
    "    xbins = np.floor((data.x.values - x_bounds[0])\n",
    "                     / delta_X).astype(np.int32)\n",
    "    ybins = np.floor((data.y.values - y_bounds[0])\n",
    "                     / delta_Y).astype(np.int32)\n",
    "\n",
    "    # some points fall on the upper/right edge of the domain\n",
    "    # tweak their index to bring them back in the box\n",
    "    xbins[xbins == NX] = NX-1\n",
    "    ybins[ybins == NY] = NY-1\n",
    "\n",
    "    xlabel = 'x_bin_{0:03d}'.format(NX)\n",
    "    ylabel = 'y_bin_{0:03d}'.format(NY)\n",
    "\n",
    "    data[xlabel] = xbins\n",
    "    data[ylabel] = ybins\n",
    "    return\n",
    "\n",
    "\n",
    "def mapkprecision(truthvalues, predictions):\n",
    "    '''\n",
    "    This is a faster implementation of MAP@k valid for numpy arrays.\n",
    "    It is only valid when there is one single truth value. \n",
    "\n",
    "    m ~ number of observations\n",
    "    k ~ MAP at k -- in this case k should equal 3\n",
    "\n",
    "    truthvalues.shape = (m,) \n",
    "    predictions.shape = (m, k)\n",
    "    '''\n",
    "    z = (predictions == truthvalues[:, None]).astype(np.float32)\n",
    "    weights = 1./(np.arange(predictions.shape[1], dtype=np.float32) + 1.)\n",
    "    z = z * weights[None, :]\n",
    "    return np.mean(np.sum(z, axis=1))\n",
    "\n",
    "\n",
    "def CV_fit(train, test, kfold=5, keep_fraction=0.5):\n",
    "    '''Performs a fit using XGBoost. Applies kfold cross validation to \n",
    "    estimate error. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train \n",
    "        The training dataset as a pandas DataFrame\n",
    "    test\n",
    "        The testing dataset as a pandas DataFrame\n",
    "    kfold\n",
    "        The number of folds to use for K Fold Validation\n",
    "    keep_fraction\n",
    "        A float between 0 and 1. The fraction of events in each bin\n",
    "        to keep while minimizing the number of place_ids in the training\n",
    "        set. Low values throw away a lot of infrequent place_ids, and\n",
    "        values near 1 retain almost all place_ids. \n",
    "    '''\n",
    "\n",
    "    # choose 10 random bins for a quicker estimate of algorithm error.\n",
    "    rs = np.random.RandomState(42)\n",
    "    #bin_numbers = zip(rs.randint(0, 50, size=NBINS), rs.randint(0, 50, size=NBINS))\n",
    "    bin_numbers[]\n",
    "    for(int i in range(0, ))\n",
    "\n",
    "    predictions = []\n",
    "    map3s = []\n",
    "\n",
    "    #Choose this line for the whole dataset.\n",
    "    #for i_bin_x, i_bin_y in itertools.product(xrange(50), xrange(50)):\n",
    "    for i_bin_x, i_bin_y in bin_numbers:\n",
    "        print(\"Bin {},{}\".format(i_bin_x, i_bin_y))\n",
    "\n",
    "        # choose the correct bin, sort values in time to better simulate\n",
    "        # the real train/test split for k-fold validation\n",
    "        train_in_bin = train[(train.x_bin_050 == i_bin_x)\n",
    "                             & (train.y_bin_050 == i_bin_y)].sort_values('time')\n",
    "        test_in_bin = test[(test.x_bin_050 == i_bin_x)\n",
    "                           & (test.y_bin_050 == i_bin_y)].sort_values('time')\n",
    "\n",
    "        N_total_in_bin = train_in_bin.shape[0]\n",
    "        keep_N = int(float(N_total_in_bin)*keep_fraction)\n",
    "        vc = train_in_bin.place_id.value_counts()\n",
    "\n",
    "        # eliminate all ids which are low enough frequency\n",
    "        vc = vc[np.cumsum(vc.values) < keep_N]\n",
    "        df1 = pd.DataFrame({'place_id': vc.index, 'freq': vc.values})\n",
    "\n",
    "        # this represents the training set after all low frequency place_ids\n",
    "        # are removed\n",
    "        train_in_bin_2 = pd.merge(train_in_bin, df1, on='place_id',\n",
    "                                  how='inner')\n",
    "\n",
    "        # XG Boost requires labels from 0 to n_labels, not place_ids\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(train_in_bin_2.place_id.values)\n",
    "        y_train = le.transform(train_in_bin_2.place_id.values)\n",
    "\n",
    "        # select columns (features) and make a numpy array\n",
    "        x_train = train_in_bin_2[feature_list].as_matrix()\n",
    "        x_test = test_in_bin[feature_list].as_matrix()\n",
    "\n",
    "        # Construct DMatrices\n",
    "        dm_train = xgboost.DMatrix(x_train, label=y_train)\n",
    "        dm_test = xgboost.DMatrix(x_test)\n",
    "\n",
    "        # use the XGBoost built in cross validation function,\n",
    "        # stopping early to prevent overfitting\n",
    "        res = xgboost.cv(\n",
    "            {'eta': 0.1, 'objective': 'multi:softprob',\n",
    "             'num_class': len(le.classes_),\n",
    "             'alpha': 0.1, 'lambda': 0.1, 'booster': 'gbtree'},\n",
    "            dm_train, num_boost_round=200, nfold=kfold, seed=42,\n",
    "            early_stopping_rounds=10, verbose_eval=10\n",
    "            # For some reason, verbose_eval seems to be broken on my install\n",
    "        )\n",
    "\n",
    "        print(res)\n",
    "\n",
    "        # this will be the number of epochs that (approximately) prevents\n",
    "        # overfitting\n",
    "        N_epochs = res.shape[0]\n",
    "\n",
    "        # For some reason, verbose_eval seems to be broken on my install\n",
    "        booster = xgboost.train(\n",
    "            {'eta': 0.1, 'objective': 'multi:softprob',\n",
    "             'num_class': len(le.classes_),\n",
    "             'alpha': 0.1, 'lambda': 0.1, 'booster': 'gbtree'},\n",
    "            dm_train, num_boost_round=N_epochs, verbose_eval=10)\n",
    "        predict_y_train = booster.predict(dm_train)\n",
    "        predict_y_test = booster.predict(dm_test)\n",
    "\n",
    "        # There is a DELIBERATE error here where the CV MAP@3 is misleadingly\n",
    "        # good as compared to the true MAP@3 on the test set. I leave it as\n",
    "        # an exercise how to fix this.\n",
    "\n",
    "        # A top k algorithm would be theoretically faster, but benchmarks\n",
    "        # indicate that with Order(50) elements, an argsort is faster than\n",
    "        # heapq.\n",
    "\n",
    "        predicted_train_idx = np.argsort(\n",
    "            predict_y_train, axis=1)[:, -3:][:, ::-1]\n",
    "        predicted_test_idx = np.argsort(\n",
    "            predict_y_test, axis=1)[:, -3:][:, ::-1]\n",
    "\n",
    "        c = np.array(le.classes_)\n",
    "        predicted_train_place_id = np.take(c, predicted_train_idx)\n",
    "        predicted_test_place_id = np.take(c, predicted_test_idx)\n",
    "\n",
    "        map3 = mapkprecision(c.take(y_train), predicted_train_place_id)\n",
    "        map3s.append(map3)\n",
    "\n",
    "        print(\"Train Cross Validated MAP@3: {0:.4f}\".format(map3))\n",
    "        result = pd.DataFrame({'row_id': test_in_bin.index,\n",
    "                               'pred_1': predicted_test_place_id[:, 0],\n",
    "                               'pred_2': predicted_test_place_id[:, 1],\n",
    "                               'pred_3': predicted_test_place_id[:, 2]})\n",
    "        predictions.append(result)\n",
    "\n",
    "    return pd.concat(predictions), map3s\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    print('Reading csv files from disk.', flush=True)\n",
    "    train = pd.read_csv('../../train.csv')\n",
    "    test = pd.read_csv('../../test.csv')\n",
    "\n",
    "    print(\"Calculating time features\", flush=True)\n",
    "\n",
    "    #train['hour'] = (train['time']//60) % 24+1  # 1 to 24\n",
    "    #test['hour'] = (test['time']//60) % 24+1  # 1 to 24\n",
    "    \n",
    "    train = prepare_data(train)\n",
    "    test = prepare_data(test)\n",
    "    \n",
    "    # You'll probably want to generate more features here.\n",
    "\n",
    "    print(\"Binning data\", flush=True)\n",
    "    horizontally_bin_data(train, 50, 50)\n",
    "    horizontally_bin_data(test, 50, 50)\n",
    "\n",
    "    print(\"Starting CV\", flush=True)\n",
    "    predictions, map3s = CV_fit(train, test, kfold=5)\n",
    "    print(\"MAP@3 CV in bins {}\".format(map3s), flush=True)\n",
    "\n",
    "    print(\"Writing xgb_submission.csv\", flush=True)\n",
    "    with open('xgb_submission.csv', 'w') as fh:\n",
    "        fh.write('row_id,place_id\\n')\n",
    "        for r in predictions.itertuples():\n",
    "            fh.write(\"{0},{1} {2} {3}\\n\".format(*r))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading csv files from disk.\n",
      "Calculating time features\n",
      "Binning data\n"
     ]
    }
   ],
   "source": [
    "print('Reading csv files from disk.', flush=True)\n",
    "train = pd.read_csv('../../train.csv')\n",
    "test = pd.read_csv('../../test.csv')\n",
    "\n",
    "print(\"Calculating time features\", flush=True)\n",
    "\n",
    "#train['hour'] = (train['time']//60) % 24+1  # 1 to 24\n",
    "#test['hour'] = (test['time']//60) % 24+1  # 1 to 24\n",
    "\n",
    "train = prepare_data(train)\n",
    "test = prepare_data(test)\n",
    "\n",
    "# You'll probably want to generate more features here.\n",
    "\n",
    "print(\"Binning data\", flush=True)\n",
    "horizontally_bin_data(train, 50, 50)\n",
    "horizontally_bin_data(test, 50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "      <th>place_id</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>log_month</th>\n",
       "      <th>x_bin_050</th>\n",
       "      <th>y_bin_050</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.7941</td>\n",
       "      <td>9.0809</td>\n",
       "      <td>1.732394</td>\n",
       "      <td>470702</td>\n",
       "      <td>8523065625</td>\n",
       "      <td>6.050000</td>\n",
       "      <td>1</td>\n",
       "      <td>329</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1.142886</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.9567</td>\n",
       "      <td>4.7968</td>\n",
       "      <td>1.113943</td>\n",
       "      <td>186555</td>\n",
       "      <td>1757726713</td>\n",
       "      <td>22.266667</td>\n",
       "      <td>6</td>\n",
       "      <td>131</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.864416</td>\n",
       "      <td>29</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>8.3078</td>\n",
       "      <td>7.0407</td>\n",
       "      <td>1.869232</td>\n",
       "      <td>322648</td>\n",
       "      <td>1137537235</td>\n",
       "      <td>10.483333</td>\n",
       "      <td>3</td>\n",
       "      <td>226</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1.019893</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7.3665</td>\n",
       "      <td>2.5165</td>\n",
       "      <td>1.812913</td>\n",
       "      <td>704587</td>\n",
       "      <td>6567393236</td>\n",
       "      <td>16.133333</td>\n",
       "      <td>2</td>\n",
       "      <td>126</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1.285780</td>\n",
       "      <td>36</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4.0961</td>\n",
       "      <td>1.1307</td>\n",
       "      <td>1.491362</td>\n",
       "      <td>472130</td>\n",
       "      <td>7440663949</td>\n",
       "      <td>5.850000</td>\n",
       "      <td>2</td>\n",
       "      <td>330</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1.143918</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id       x       y  accuracy    time    place_id       hour  weekday  \\\n",
       "0       0  0.7941  9.0809  1.732394  470702  8523065625   6.050000        1   \n",
       "1       1  5.9567  4.7968  1.113943  186555  1757726713  22.266667        6   \n",
       "2       2  8.3078  7.0407  1.869232  322648  1137537235  10.483333        3   \n",
       "3       3  7.3665  2.5165  1.812913  704587  6567393236  16.133333        2   \n",
       "4       4  4.0961  1.1307  1.491362  472130  7440663949   5.850000        2   \n",
       "\n",
       "   day  month  year  log_month  x_bin_050  y_bin_050  \n",
       "0  329     11     1   1.142886          3         45  \n",
       "1  131      5     1   0.864416         29         23  \n",
       "2  226      8     1   1.019893         41         35  \n",
       "3  126      5     2   1.285780         36         12  \n",
       "4  330     11     1   1.143918         20          5  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
